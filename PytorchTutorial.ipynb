{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation \n",
    "#### Anaconda 3-4.2.0 (or higher)\n",
    "#### Create virtual environment >> conda create -n yourenvname\n",
    "#### Activate virtual environment >>  conda activate yourenvname\n",
    "#### Install torchvision (Installs pytorch too) >> conda install -c pytorch torchvision\n",
    "#### Open Jupyter notebook >> jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2 as cv\n",
    "from statistics import mode\n",
    "# from skimage import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO-DO LIST\n",
    "\n",
    "1. Make test and train sets \n",
    "    - image folders and label csvs\n",
    "    - make transform\n",
    "    - invoke customDataset to make test n train sets\n",
    "\n",
    "\n",
    "\n",
    "2. Set CNN parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class convention\n",
    "\n",
    "0 - Forward\n",
    "1 - Backward\n",
    "2 - Stop\n",
    "3 - Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Transformation\n",
    "#### Convert the Numpy arrays to PyTorch tensors and normalize input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = (51,51)\n",
    "GaussianSigma = 40\n",
    "\n",
    "def rgbEqualize(img):\n",
    "    img_yuv = cv.cvtColor(img, cv.COLOR_BGR2YUV)\n",
    "\n",
    "    # equalize the histogram of the Y channel\n",
    "    img_yuv[:,:,0] = cv.equalizeHist(img_yuv[:,:,0])\n",
    "\n",
    "    # convert the YUV image back to RGB format\n",
    "    img_output = cv.cvtColor(img_yuv, cv.COLOR_YUV2BGR)\n",
    "    \n",
    "    return img_output\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset\n",
    "#### This is a built-in demo. Creating your own custom dataloader is discussed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class liveDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images, transform=None):\n",
    "        self.data = images\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # load image as ndarray type (Height * Width * Channels)\n",
    "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
    "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
    "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
    "        image = self.data[index].values.astype(np.uint8).reshape((3,50,50))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kagDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, file_path, transform=None):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # load image as ndarray type (Height * Width * Channels)\n",
    "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
    "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
    "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
    "        image = self.data.iloc[index, 1:].values.astype(np.uint8).reshape((3,50,50))\n",
    "        label = self.data.iloc[index, 0]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset = datasets.MNIST('~/Documents/Sem9/TA-COL780/Pytorch_tutorial/', download=True, train=True, transform=transform)\n",
    "# testset = datasets.MNIST('~/Documents/Sem9/TA-COL780/Pytorch_tutorial/', download=True, train=False, transform=transform)\n",
    "\n",
    "train_path = './data/train/train.csv'\n",
    "test_path = './data/test/test.csv'\n",
    "\n",
    "\n",
    "\n",
    "trainset = kagDataset(train_path)\n",
    "testset = kagDataset(test_path)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=20, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=20, shuffle=True)\n",
    "classes = ('0','1','2','3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 50, 50])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(images[0].numpy().squeeze());  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure = plt.figure()\n",
    "# num_of_images = 15\n",
    "# for index in range(1, num_of_images + 1):\n",
    "#     plt.subplot(6, 10, index)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 11 * 11, 120)  \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=1936, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 1.443\n",
      "[1,    40] loss: 0.577\n",
      "[1,    60] loss: 0.332\n",
      "[2,    20] loss: 0.131\n",
      "[2,    40] loss: 0.065\n",
      "[2,    60] loss: 0.075\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mini_batch = 20\n",
    "loss_values = []\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # make the parameter gradients zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        inputs=inputs.float()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % mini_batch == mini_batch-1:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / mini_batch))\n",
    "            loss_values.append(running_loss/mini_batch)\n",
    "            running_loss = 0.0\n",
    "        \n",
    "plt.plot(loss_values)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0     1     3     1\n"
     ]
    }
   ],
   "source": [
    "# get some training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:      0     2     3     0\n"
     ]
    }
   ],
   "source": [
    "# get some test images\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Train and Test accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 99 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        images = images.float()\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the train images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 52 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.float()\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video dimensions are: 1280 x 720 \n"
     ]
    }
   ],
   "source": [
    "vid = cv.VideoCapture(0)\n",
    "\n",
    "print(\"Video dimensions are: %d x %d \" %(int(vid.get(3)), int(vid.get(4))))\n",
    "\n",
    "backSubMOG = cv.bgsegm.createBackgroundSubtractorMOG()\n",
    "\n",
    "input_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_width=1920\n",
    "frame_height=1080\n",
    "font = cv.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "while True:\n",
    "    ret, frame = vid.read()\n",
    "    if ret==True:\n",
    "#         gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "        subbedFrameMOG = backSubMOG.apply(frame)\n",
    "        \n",
    "        \n",
    "#         cv.imshow('MOG', subbedFrameMOG)\n",
    "\n",
    "\n",
    "#         frame = cv.resize(gray, (50,50), interpolation = cv.INTER_AREA)\n",
    "    \n",
    "      \n",
    "        frame = rgbEqualize(frame)\n",
    "        \n",
    "        blur_frame = cv.GaussianBlur(frame, kernel_size, GaussianSigma)\n",
    "        blur_frame = cv.resize(blur_frame, (50,50), interpolation = cv.INTER_AREA)\n",
    "\n",
    "        \n",
    "        \n",
    "        blur_displayFrame = cv.resize(blur_frame, (600,600), interpolation = cv.INTER_AREA)\n",
    "        blur_displayFrame = blur_displayFrame[:, ::-1]\n",
    "        \n",
    "#         cv.imshow('original_blur', blur_displayFrame)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        input_data.append(blur_frame.flatten().tolist())\n",
    "        finalLabel = -1\n",
    "\n",
    "        if len(input_data) == 20:\n",
    "            testset = liveDataset(input_data)\n",
    "            test_loader = torch.utils.data.DataLoader(testset, batch_size=20, shuffle=False)\n",
    "           \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for data in testloader:\n",
    "                    images, _ = data\n",
    "                    images = images.float()\n",
    "                    outputs = net(images)\n",
    "                    \n",
    "                    try:\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "#                         print(predicted)\n",
    "\n",
    "                        labelCount = np.array([0,0,0,0])\n",
    "                        for i in predicted:\n",
    "                            labelCount[predicted[i]]+=1\n",
    "\n",
    "                        finalLabel = np.argmax(labelCount)\n",
    "                    except:\n",
    "                        finalLabel = 4\n",
    "#                     print(finalLabel)\n",
    "            blur_displayFrame = cv.putText(blur_displayFrame, str(finalLabel), (200,200), font, 3, (0,255,0),10,cv.LINE_AA)\n",
    "            input_data = []\n",
    "            cv.imshow('original_blur', blur_displayFrame)\n",
    "\n",
    "\n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "vid.release()\n",
    "cv.destroyAllWindows()\n",
    "cv.waitKey(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making your own custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.label = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.label.iloc[idx, 0])\n",
    "#         print(img_name)\n",
    "        image = cv.imread(img_name)\n",
    "        labels = self.label.iloc[idx, 1]\n",
    "        labels = np.array([labels])\n",
    "        labels = labels.astype('float').reshape(-1, 1)\n",
    "        sample = {'image': image, 'label': labels}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_dataset = customDataset(csv_file='labels.csv',\n",
    "#                                     root_dir='/home/surya/Documents/Sem9/TA-COL780/Pytorch_tutorial/images/')\n",
    "\n",
    "# for i in range(len(custom_dataset)):\n",
    "#     sample = custom_dataset[i]\n",
    "\n",
    "#     print(i, sample['image'].shape, sample['label'].shape, sample['label'][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
